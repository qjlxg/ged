import pandas as pd
import os
import glob
import numpy as np
from datetime import datetime
from multiprocessing import Pool, cpu_count

# ==============================================================================
# æˆ˜æ³•è¯´æ˜ï¼šAlpha Hunter V7 å®˜æ–¹å†³ç­–å…¨åŠŸèƒ½ç‰ˆ
# 1. [åˆ¤å®šä¾æ®]ï¼šRSI(è¶…ä¹°è¶…å–)ã€BIAS(å‡çº¿åç¦»)ã€VOLUME(é‡ä»·éªŒè¯)ã€ATR(æ³¢åŠ¨é€‚é…)
# 2. [å†å²è®°è´¦]ï¼šè‡ªåŠ¨ç»´æŠ¤ signal_tracker.csvï¼Œè®¡ç®— T+7/14/20/60 çœŸå®èƒœç‡
# 3. [å†³ç­–å­˜æ¡£]ï¼šæ¯æ—¥å»ºè®®è‡ªåŠ¨è¿½åŠ åˆ° final_decision_log.csvï¼Œæ–¹ä¾¿æŸ¥é˜…å†å²
# ==============================================================================

DATA_DIR = 'fund_data'
ETF_LIST_FILE = 'ETFåˆ—è¡¨.xlsx' 
TRACKER_FILE = 'signal_tracker.csv'    # èƒœç‡å›æµ‹è´¦æœ¬
DECISION_LOG = 'final_decision_log.csv' # æ¯æ—¥ä¹°å–å†³ç­–è®°å½•

def calculate_rsi(series, period=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/period, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/period, adjust=False).mean()
    rs = gain / (loss + 1e-9)
    return 100 - (100 / (1 + rs))

def analyze_fund(file_path):
    try:
        full_df = pd.read_csv(file_path, encoding='utf-8-sig')
        if len(full_df) < 60: return None
        full_df.columns = [c.strip() for c in full_df.columns]
        df = full_df.tail(120).copy()
        latest = df.iloc[-1]
        
        # --- [ç¡¬æ ¸æŒ‡æ ‡è®¡ç®—] ---
        # 1. æƒ…ç»ªæ¸©åº¦ (RSI)
        rsi_val = calculate_rsi(df['æ”¶ç›˜']).iloc[-1]
        # 2. å‡çº¿åç¦»åº¦ (BIAS)
        ma20 = df['æ”¶ç›˜'].rolling(20).mean().iloc[-1]
        bias = (latest['æ”¶ç›˜'] - ma20) / ma20 * 100
        # 3. äººæ°”é‡èƒ½ (VOLUME)
        vol_ratio = df['æˆäº¤é¢'].tail(5).mean() / (df['æˆäº¤é¢'].tail(20).mean() + 1e-9)
        # 4. æ³¢åŠ¨æ€§æ ¼ (ATR)
        atr = (df['æœ€é«˜'] - df['æœ€ä½']).rolling(14).mean().iloc[-1]
        volatility = (atr / latest['æ”¶ç›˜']) * 100

        # --- [å†³ç­–é€»è¾‘ç³»ç»Ÿ] ---
        signal_type, reason, is_buy = "è§‚æœ›", "æ­£å¸¸æ³¢åŠ¨", False

        # ä¹°å…¥ä¾æ®ï¼šæåº¦å†·æ¸… + ä»·æ ¼è·Œç ´ä½
        if rsi_val < 68:#rsi_val < 38
            signal_type, reason, is_buy = "å»ºè®®ä¹°å…¥", "æƒ…ç»ªå†°ç‚¹/ä½ä½å»ºä»“", True
            if rsi_val < 32: reason = "ä¸¥é‡è¶…è·Œ/é»„é‡‘åº•"
        # å–å‡ºä¾æ®ï¼šæƒ…ç»ªè¿‡çƒ­ æˆ– ç¼©é‡æ‹‰å‡ï¼ˆè¯±å¤šï¼‰
        elif rsi_val > 70:
            signal_type, reason = "å»ºè®®å–å‡º", "æƒ…ç»ªè¿‡çƒ­/é€¢é«˜æ­¢ç›ˆ"
        elif latest['æ”¶ç›˜'] > ma20 and vol_ratio < 0.8:
            signal_type, reason = "å»ºè®®å–å‡º", "é‡ä»·èƒŒç¦»/è­¦æƒ•è¯±å¤š"

        code = os.path.basename(file_path).replace('.csv', '')
        return {
            'analysis': {
                'æ—¥æœŸ': latest['æ—¥æœŸ'] if 'æ—¥æœŸ' in latest else datetime.now().strftime('%Y-%m-%d'),
                'ä»£ç ': code, 'ä»·æ ¼': latest['æ”¶ç›˜'], 'RSI': round(rsi_val, 1), 
                'ä¿¡å·': signal_type, 'ç†ç”±': reason, 'is_signal': is_buy,
                'åç¦»åº¦%': round(bias, 2), 'äººæ°”å€¼': round(vol_ratio, 2)
            },
            'history': full_df[['æ—¥æœŸ', 'æ”¶ç›˜']]
        }
    except: return None

def update_tracker(new_results, hist_map):
    """å†å²èƒœç‡å›æµ‹è´¦æœ¬ç»´æŠ¤"""
    cols = ['ä»£ç ', 'å…¥åœºæ—¥æœŸ', 'ä¹°å…¥ä»·', 'T+7æ”¶ç›Š%', 'T+14æ”¶ç›Š%', 'T+20æ”¶ç›Š%', 'T+60æ”¶ç›Š%', 'çŠ¶æ€']
    tracker = pd.read_csv(TRACKER_FILE) if os.path.exists(TRACKER_FILE) else pd.DataFrame(columns=cols)
    
    # è®°å½•æ–°ä¹°å…¥ä¿¡å·
    for item in new_results:
        if item['is_signal']:
            recent = tracker[tracker['ä»£ç '] == item['ä»£ç ']].tail(1)
            if recent.empty or (datetime.now() - pd.to_datetime(recent['å…¥åœºæ—¥æœŸ'].values[0])).days > 10:
                new_row = pd.DataFrame([[item['ä»£ç '], item['æ—¥æœŸ'], item['ä»·æ ¼'], np.nan, np.nan, np.nan, np.nan, 'æŒæœ‰ä¸­']], columns=cols)
                tracker = pd.concat([tracker, new_row], ignore_index=True)

    # åˆ·æ–°å·²è®°å½•ä¿¡å·çš„æ”¶ç›Š
    for idx, row in tracker.iterrows():
        code = str(row['ä»£ç ']).zfill(6)
        if code in hist_map:
            h_df = hist_map[code].copy()
            h_df['æ—¥æœŸ'] = pd.to_datetime(h_df['æ—¥æœŸ'])
            buy_dt = pd.to_datetime(row['å…¥åœºæ—¥æœŸ'])
            future = h_df[h_df['æ—¥æœŸ'] > buy_dt]
            if not future.empty:
                for t in [7, 14, 20, 60]:
                    col = f'T+{t}æ”¶ç›Š%'
                    if pd.isna(row[col]) and len(future) >= t:
                        p_t = future.iloc[t-1]['æ”¶ç›˜']
                        tracker.at[idx, col] = round((p_t - row['ä¹°å…¥ä»·']) / row['ä¹°å…¥ä»·'] * 100, 2)
                if len(future) >= 60: tracker.at[idx, 'çŠ¶æ€'] = 'å·²ç»“é¡¹'
    
    tracker.to_csv(TRACKER_FILE, index=False, encoding='utf-8-sig')
    return tracker

def main():
    # 1. èµ„æºå‡†å¤‡
    target_file = ETF_LIST_FILE if os.path.exists(ETF_LIST_FILE) else ETF_LIST_FILE.replace('.xlsx', '.csv')
    try:
        if target_file.endswith('.xlsx'): name_df = pd.read_excel(target_file)
        else: name_df = pd.read_csv(target_file, encoding='utf-8-sig')
        name_df.columns = [c.strip() for c in name_df.columns]
        name_map = dict(zip(name_df['è¯åˆ¸ä»£ç '].astype(str).str.zfill(6), name_df['è¯åˆ¸ç®€ç§°']))
    except: return

    # 2. å¹¶è¡Œæ‰«æ
    csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    print(f"ğŸš€ Alpha Hunter å¯åŠ¨ï¼šæ·±åº¦è¯Šæ–­ {len(csv_files)} ä¸ªå“ç§...")
    with Pool(cpu_count()) as p:
        raw_output = p.map(analyze_fund, csv_files)
    
    results = [r['analysis'] for r in raw_output if r and r['analysis']['ä»£ç '] in name_map]
    hist_map = {r['analysis']['ä»£ç ']: r['history'] for r in raw_output if r}

    # 3. è®°è´¦ä¸æŒä¹…åŒ–
    tracker_df = update_tracker(results, hist_map)
    
    # æ•´ç†å†³ç­–è®°å½•
    decisions = [r for r in results if r['ä¿¡å·'] in ["å»ºè®®ä¹°å…¥", "å»ºè®®å–å‡º"]]
    if decisions:
        log_df = pd.DataFrame(decisions)
        log_df['ç®€ç§°'] = log_df['ä»£ç '].apply(lambda x: name_map.get(x, 'æœªçŸ¥'))
        # è°ƒæ•´åˆ—é¡ºåºä¿å­˜
        save_cols = ['æ—¥æœŸ', 'ä»£ç ', 'ç®€ç§°', 'ä»·æ ¼', 'ä¿¡å·', 'ç†ç”±', 'RSI', 'åç¦»åº¦%', 'äººæ°”å€¼']
        log_df = log_df[save_cols]
        header = not os.path.exists(DECISION_LOG)
        log_df.to_csv(DECISION_LOG, mode='a', index=False, header=header, encoding='utf-8-sig')

    # 4. æ§åˆ¶å°ç²¾ç®€è¾“å‡º
    print(f"\nğŸ“… è¯Šæ–­æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}")
    print(f"ğŸ’¾ å†³ç­–è®°å½•å·²æ›´æ–°è‡³: {DECISION_LOG}")
    print("=" * 80)
    
    buy_list = [r for r in results if r['ä¿¡å·'] == "å»ºè®®ä¹°å…¥"]
    sell_list = [r for r in results if r['ä¿¡å·'] == "å»ºè®®å–å‡º"]

    print("ğŸŸ¢ ã€å»ºè®®ä¹°å…¥æ¸…å•ã€‘")
    if buy_list:
        for r in sorted(buy_list, key=lambda x: x['RSI']):
            print(f"  {r['ä»£ç ']} | {name_map[r['ä»£ç ']]:<12} | ç°ä»·:{r['ä»·æ ¼']:<7} | {r['ç†ç”±']}")
    else: print("  (å¸‚åœºç«çƒ­ï¼Œæš‚æ— ä½å¸æœºä¼š)")

    print("-" * 80)
    print("ğŸ”´ ã€å»ºè®®å–å‡ºæ¸…å•ã€‘")
    if sell_list:
        for r in sorted(sell_list, key=lambda x: x['RSI'], reverse=True):
            print(f"  {r['ä»£ç ']} | {name_map[r['ä»£ç ']]:<12} | ç°ä»·:{r['ä»·æ ¼']:<7} | {r['ç†ç”±']}")
    else: print("  (æš‚æ— é«˜é£é™©æŠ›å”®å“ç§)")
    
    print("=" * 80)

    # 5. æ‰“å°èƒœç‡ç»Ÿè®¡ç®€æŠ¥
    print("\nğŸ“Š å†å²ä¿¡å·å¯é æ€§éªŒè¯ (åŸºäºå†å²æ¨¡æ‹Ÿä¹°å…¥è®°å½•):")
    for t in [7, 14, 20, 60]:
        col = f'T+{t}æ”¶ç›Š%'
        if col in tracker_df.columns:
            valid = tracker_df[tracker_df[col].notna()]
            if not valid.empty:
                wr = (valid[col].astype(float) > 0).sum() / len(valid) * 100
                print(f" >> T+{t}å¤© èƒœç‡: {wr:.1f}% | æ ·æœ¬é‡: {len(valid)}")
            else: print(f" >> T+{t}å¤© æ ·æœ¬æ”¶é›†é˜¶æ®µ...")

if __name__ == "__main__":
    main()
